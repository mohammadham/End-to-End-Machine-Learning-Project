{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMawaA6aP1Xc7wGvQodtJhv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadham/End-to-End-Machine-Learning-Project/blob/main/End_to_End_Machine_Learning_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "excersise 1:In this code, we're using the \n",
        "fetch_california_housing\n",
        " function from scikit-learn to load the California housing dataset. We then split the data into training and test sets using the \n",
        "train_test_split\n",
        " function, which randomly splits the data into two sets based on the \n",
        "test_size\n",
        " parameter (in this case, 20% of the data is used for testing)."
      ],
      "metadata": {
        "id": "V_A2cwH67iAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoAIzzocQSdR"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = [\n",
        "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]},\n",
        "    {'kernel': ['rbf'], 'C': [0.1, 1, 10, 100, 1000], 'gamma': [0.1, 0.01, 0.001, 0.0001]},\n",
        "]\n",
        "\n",
        "# Create an SVR model\n",
        "svr = SVR()\n",
        "\n",
        "# Use GridSearchCV to search for the best hyperparameters\n",
        "grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding RMSE score\n",
        "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "print(\"Best RMSE score: \", np.sqrt(-grid_search.best_score_))\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "final_model = grid_search.best_estimator_\n",
        "y_pred = final_model.predict(X_test)\n",
        "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Final RMSE score: \", final_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercise 2: We define a dictionary of parameter distributions to search over, which includes different distributions for the C and gamma hyperparameters for the 'rbf' kernel, as well as the 'linear' kernel.\n",
        "\n",
        "We create an SVR model and use \n",
        "RandomizedSearchCV\n",
        " to search over the parameter distributions for the best hyperparameters, using 5-fold cross-validation and the negative mean squared error as the scoring metric. After fitting the \n",
        "RandomizedSearchCV\n",
        " object to the training data, we print out the best hyperparameters and corresponding RMSE score.\n",
        "\n",
        "Finally, we evaluate the best model on the test set and print out the final RMSE score.\n"
      ],
      "metadata": {
        "id": "Wadh2k4885_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import expon, reciprocal\n",
        "import numpy as np\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter distributions to search\n",
        "param_distribs = {\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'C': reciprocal(20, 200000),\n",
        "    'gamma': expon(scale=1.0),\n",
        "}\n",
        "\n",
        "# Create an SVR model\n",
        "svr = SVR()\n",
        "\n",
        "# Use RandomizedSearchCV to search for the best hyperparameters\n",
        "rnd_search = RandomizedSearchCV(svr, param_distributions=param_distribs, n_iter=50, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "rnd_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding RMSE score\n",
        "print(\"Best hyperparameters: \", rnd_search.best_params_)\n",
        "print(\"Best RMSE score: \", np.sqrt(-rnd_search.best_score_))\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "final_model = rnd_search.best_estimator_\n",
        "y_pred = final_model.predict(X_test)\n",
        "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Final RMSE score: \", final_rmse)"
      ],
      "metadata": {
        "id": "FcCaRukW9jAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "exercise 3 : \n",
        "We define the numerical and categorical attributes, and then define the preparation pipeline using \n",
        "ColumnTransformer\n",
        " to apply the \n",
        "num_pipeline\n",
        " to the numerical attributes and pass through the categorical attributes.\n",
        "\n",
        "We fit a \n",
        "RandomForestRegressor\n",
        " to the training data to compute feature importances, and then define a transformer \n",
        "TopFeatureSelector\n",
        " to select only the top k features based on the feature importances.\n",
        "\n",
        "We define a new pipeline \n",
        "preparation_and_feature_selection_pipeline\n",
        " that includes the feature selection transformer, and fit and transform the training data using this pipeline. We then print out the top k feature indices and names, and double check that the selected features match the top k features."
      ],
      "metadata": {
        "id": "YRRjKdns_NF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the numerical and categorical attributes\n",
        "num_attribs = list(housing.feature_names)\n",
        "cat_attribs = []\n",
        "\n",
        "# Define the preparation pipeline\n",
        "num_pipeline = Pipeline([\n",
        "    ('std_scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "    ('num', num_pipeline, num_attribs),\n",
        "    ('cat', 'passthrough', cat_attribs),\n",
        "])\n",
        "\n",
        "# Fit a RandomForestRegressor to the training data to compute feature importances\n",
        "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest_reg.fit(X_train, y_train)\n",
        "feature_importances = forest_reg.feature_importances_\n",
        "\n",
        "# Define a transformer to select only the top k features\n",
        "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, feature_importances, k):\n",
        "        self.feature_importances = feature_importances\n",
        "        self.k = k\n",
        "    def fit(self, X, y=None):\n",
        "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[:, self.feature_indices_]\n",
        "\n",
        "def indices_of_top_k(arr, k):\n",
        "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
        "\n",
        "# Define the number of top features we want to keep\n",
        "k = 5\n",
        "\n",
        "# Define a new pipeline that includes the feature selection transformer\n",
        "preparation_and_feature_selection_pipeline = Pipeline([\n",
        "    ('preparation', full_pipeline),\n",
        "    ('feature_selection', TopFeatureSelector(feature_importances, k))\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data and transform the data\n",
        "housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(X_train)\n",
        "\n",
        "# Print the top k feature indices and names\n",
        "top_k_feature_indices = indices_of_top_k(feature_importances, k)\n",
        "print(\"Top k feature indices: \", top_k_feature_indices)\n",
        "print(\"Top k feature names: \", np.array(num_attribs)[top_k_feature_indices])\n",
        "\n",
        "# Double check that the selected features match the top k features\n",
        "print(\"Selected features for first 3 instances: \", housing_prepared_top_k_features[:3, :])\n",
        "print(\"Top k features for first 3 instances: \", X_train[:3, top_k_feature_indices])"
      ],
      "metadata": {
        "id": "nH0See4P_jWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "excersise 4:\n",
        "We define the numerical and categorical attributes, and then define the preparation pipeline using \n",
        "ColumnTransformer\n",
        " to apply the \n",
        "num_pipeline\n",
        " to the numerical attributes and pass through the categorical attributes.\n",
        "\n",
        "We fit a \n",
        "RandomForestRegressor\n",
        " to the training data to compute feature importances, and then define a transformer \n",
        "TopFeatureSelector\n",
        " to select only the top k features based on the feature importances.\n",
        "\n",
        "We define the parameter distributions to search for the best hyperparameters for an SVR model, and use \n",
        "RandomizedSearchCV\n",
        " to search for the best hyperparameters.\n",
        "\n",
        "We define the final pipeline \n",
        "prepare_select_and_predict_pipeline\n",
        " that includes data preparation, feature selection, and prediction using the best SVR model found by \n",
        "RandomizedSearchCV\n",
        ". We fit the pipeline to the full housing dataset and make predictions on some test data.\n",
        "\n",
        "Finally, we print out the predictions and labels for the test data."
      ],
      "metadata": {
        "id": "rTsx9K3bAMAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the numerical and categorical attributes\n",
        "num_attribs = list(housing.feature_names)\n",
        "cat_attribs = []\n",
        "\n",
        "# Define the preparation pipeline\n",
        "num_pipeline = Pipeline([\n",
        "    ('std_scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "    ('num', num_pipeline, num_attribs),\n",
        "    ('cat', 'passthrough', cat_attribs),\n",
        "])\n",
        "\n",
        "# Fit a RandomForestRegressor to the training data to compute feature importances\n",
        "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest_reg.fit(X_train, y_train)\n",
        "feature_importances = forest_reg.feature_importances_\n",
        "\n",
        "# Define a transformer to select only the top k features\n",
        "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, feature_importances, k):\n",
        "        self.feature_importances = feature_importances\n",
        "        self.k = k\n",
        "    def fit(self, X, y=None):\n",
        "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[:, self.feature_indices_]\n",
        "\n",
        "def indices_of_top_k(arr, k):\n",
        "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
        "\n",
        "# Define the parameter distributions to search\n",
        "param_distribs = {\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'C': reciprocal(20, 200000),\n",
        "    'gamma': expon(scale=1.0),\n",
        "}\n",
        "\n",
        "# Use RandomizedSearchCV to search for the best hyperparameters for an SVR model\n",
        "svm_reg = SVR()\n",
        "rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs, n_iter=50, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "rnd_search.fit(X_train, y_train)\n",
        "\n",
        "# Define the final pipeline that includes data preparation, feature selection, and prediction\n",
        "prepare_select_and_predict_pipeline = Pipeline([\n",
        "    ('preparation', full_pipeline),\n",
        "    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
        "    ('svm_reg', SVR(**rnd_search.best_params_)),\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data and make predictions on some test data\n",
        "prepare_select_and_predict_pipeline.fit(housing.data, housing.target)\n",
        "some_data = housing.data[:4]\n",
        "some_labels = housing.target[:4]\n",
        "print(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\n",
        "print(\"Labels:\\t\\t\", list(some_labels))"
      ],
      "metadata": {
        "id": "72V026Q2EgUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "excersise 5: \n",
        "We define the numerical and categorical attributes, and then define the preparation pipeline using \n",
        "ColumnTransformer\n",
        " to apply the \n",
        "num_pipeline\n",
        " to the numerical attributes and \n",
        "OneHotEncoder\n",
        " to the categorical attributes.\n",
        "\n",
        "We fit a \n",
        "RandomForestRegressor\n",
        " to the training data to compute feature importances, and then define a transformer \n",
        "TopFeatureSelector\n",
        " to select only the top k features based on the feature importances.\n",
        "\n",
        "We define the parameter grid to search for the best hyperparameters for the final pipeline, which includes data preparation, feature selection, and prediction using an SVR model.\n",
        "\n",
        "We use \n",
        "GridSearchCV\n",
        " to search for the best hyperparameters, and print out the best hyperparameters found by \n",
        "GridSearchCV\n",
        "."
      ],
      "metadata": {
        "id": "OYaCPXHMTdpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy.stats import reciprocal, expon\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the numerical and categorical attributes\n",
        "num_attribs = list(housing.feature_names)\n",
        "cat_attribs = ['ocean_proximity']\n",
        "\n",
        "# Define the preparation pipeline\n",
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('std_scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "    ('num', num_pipeline, num_attribs),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_attribs),\n",
        "])\n",
        "\n",
        "# Fit a RandomForestRegressor to the training data to compute feature importances\n",
        "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest_reg.fit(X_train, y_train)\n",
        "feature_importances = forest_reg.feature_importances_\n",
        "\n",
        "# Define a transformer to select only the top k features\n",
        "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, feature_importances, k):\n",
        "        self.feature_importances = feature_importances\n",
        "        self.k = k\n",
        "    def fit(self, X, y=None):\n",
        "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[:, self.feature_indices_]\n",
        "\n",
        "def indices_of_top_k(arr, k):\n",
        "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = [{\n",
        "    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
        "    'feature_selection__k': list(range(1, len(feature_importances) + 1))\n",
        "}]\n",
        "\n",
        "# Define the final pipeline that includes data preparation, feature selection, and prediction using an SVR model\n",
        "prepare_select_and_predict_pipeline = Pipeline([\n",
        "    ('preparation', full_pipeline),\n",
        "    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
        "    ('svm_reg', SVR()),\n",
        "])\n",
        "\n",
        "# Use GridSearchCV to search for the best hyperparameters\n",
        "grid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n",
        "                                scoring='neg_mean_squared_error', verbose=2)\n",
        "grid_search_prep.fit(housing.data, housing.target)\n",
        "\n",
        "# Print the best hyperparameters found by GridSearchCV\n",
        "print(\"Best hyperparameters: \", grid_search_prep.best_params_)"
      ],
      "metadata": {
        "id": "IBKYjWcCToLd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}